---
title: "Local LLM Server on 3× RTX 2070 — Architecture, Quantization, Deploy"
publishedAt: "2025-10-20T10:20:00Z"
summary: "Designing a reliable, OpenAI‑compatible LLM stack on three 8‑GB GPUs. Covers drivers, runtimes (vLLM, llama.cpp, TGI), model choices, sharding, and ops."
images: []
tag: "LLM"
---

## Prerequisites

- Hardware: single box with 3× RTX 2070 (8 GB each), ≥ 8‑core CPU, ≥ 32 GB RAM, ≥ 250 GB NVMe SSD
- OS/Drivers: Ubuntu 22.04/24.04, NVIDIA driver ≥ 535, CUDA 12.x, cuDNN 9
- Containers: Docker Engine ≥ 26 with Compose plugin, `nvidia-container-toolkit`
- Tooling: Python 3.11+ (for conversion scripts), `git`, `curl`
- Storage: 50–150 GB free for models and caches
- Network: outbound internet for model pulls; open ports 8000 (vLLM) / 8080 (TGI) or behind a reverse proxy
- Accounts: optional Hugging Face token for gated model downloads

## Constraints & Goals

- GPUs: 3× RTX 2070 (8 GB each, Turing, CC 7.5), no NVLink.
- Goals: low-latency single-user chat, decent multi-user throughput, OpenAI API compatibility.

### Architecture Diagram

<svg viewBox="0 0 900 360" role="img" aria-label="Local LLM server architecture" style={{width:'100%', maxWidth:900}}>
  <defs>
    <style>
      {`.box{fill:transparent;stroke:currentColor;stroke-width:2;rx:8;}
        .label{font: 14px ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto; fill: currentColor;}
        .arrow{stroke:currentColor;stroke-width:2;marker-end:url(#a);} `}
    </style>
    <marker id="a" markerWidth="10" markerHeight="10" refX="6" refY="3" orient="auto">
      <path d="M0,0 L0,6 L6,3 z" fill="currentColor" />
    </marker>
  </defs>
  <rect class="box" x="30" y="40" width="140" height="60" />
  <text class="label" x="100" y="75" text-anchor="middle">Clients</text>

  <rect class="box" x="210" y="40" width="180" height="60" />
  <text class="label" x="300" y="75" text-anchor="middle">Reverse Proxy (TLS, Auth)</text>
  <line class="arrow" x1="170" y1="70" x2="210" y2="70" />

  <rect class="box" x="430" y="30" width="200" height="80" />
  <text class="label" x="530" y="60" text-anchor="middle">vLLM / TGI (OpenAI API)</text>
  <line class="arrow" x1="390" y1="70" x2="430" y2="70" />

  <g>
    <rect class="box" x="660" y="20" width="200" height="100" />
    <text class="label" x="760" y="50" text-anchor="middle">GPUs</text>
    <text class="label" x="700" y="90">GPU0 (8GB)</text>
    <text class="label" x="760" y="90">GPU1 (8GB)</text>
    <text class="label" x="820" y="90">GPU2 (8GB)</text>
  </g>
  <line class="arrow" x1="630" y1="70" x2="660" y2="70" />

  <rect class="box" x="430" y="140" width="200" height="60" />
  <text class="label" x="530" y="175" text-anchor="middle">llama.cpp (fallback)</text>
  <line class="arrow" x1="510" y1="110" x2="510" y2="140" />

  <rect class="box" x="430" y="230" width="200" height="60" />
  <text class="label" x="530" y="265" text-anchor="middle">Prometheus / Grafana</text>
  <line class="arrow" x1="530" y1="190" x2="530" y2="230" />
</svg>

## Hardware/OS Baseline

- Ubuntu 22.04/24.04, NVIDIA driver ≥ 535, CUDA 12.x, cuDNN 9.
- `nvidia-smi` healthy; persistence mode on.

```bash
sudo nvidia-smi -pm 1
sudo nvidia-smi -L
```

## Runtimes You Can Use

1) vLLM (CUDA) — best throughput, tensor parallel across GPUs; supports AWQ/GPTQ.
2) Text Generation Inference (TGI) — production server with token streaming.
3) llama.cpp (GGUF) — CPU‑first with GPU offload; great fallback/low VRAM.

## Model Sizing for 3×8 GB

- 7B FP16 fits on a single 8‑GB card only with aggressive offload; better: 7B in 8‑bit or AWQ.
- 13B/14B runs via AWQ/GPTQ on vLLM with tensor parallelism of 2–3.
- Don’t chase 70B sharding here—PCIe bandwidth becomes the bottleneck.

Recommended family: Qwen2.5‑14B‑Instruct (AWQ), Mistral‑Nemo‑12B‑Instruct (AWQ).

## vLLM (Docker Compose)

```yaml
version: "3.8"
services:
  vllm:
    image: vllm/vllm-openai:latest
    command: [
      "--model", "/models/qwen2.5-14b-instruct-awq",
      "--dtype", "auto",
      "--tensor-parallel-size", "3",
      "--gpu-memory-utilization", "0.92",
      "--max-model-len", "8192",
      "--enforce-eager" # stable on older GPUs
    ]
    environment:
      - VLLM_ATTENTION_BACKEND=FLASHATTN
    ports: ["8000:8000"]
    volumes:
      - /srv/models:/models:ro
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    runtime: nvidia
```

Query with OpenAI SDKs:

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"qwen2.5-14b-instruct-awq","messages":[{"role":"user","content":"Explain KV cache"}]}'
```

## TGI Alternative

```bash
docker run --gpus all -p 8080:80 \
  -v /srv/models:/data ghcr.io/huggingface/text-generation-inference:2.1 \
  --model-id /data/qwen2.5-14b-instruct-gptq --num-shard 3
```

## llama.cpp (GGUF, CPU+GPU offload)

```bash
./llama-server -m /srv/models/llama-3.2-7b-instruct.Q5_K_M.gguf \
  --port 8081 --embedding --parallel 2 --ngl 50
```

## Scheduling & Batching

- vLLM paged attention gives large speedups; keep request batch ≤ 8 for 2070s.
- Set `gpu-memory-utilization` ~0.9 and monitor OOMs.

## Observability

- Export Prometheus metrics from vLLM; scrape with Grafana.
- Log prompts/latency; sample 1% for full traces.

## Systemd Unit (vLLM)

```ini
[Unit]
Description=vLLM OpenAI Server
After=network.target docker.service

[Service]
Restart=always
ExecStart=/usr/bin/docker compose -f /srv/llm/docker-compose.yml up --remove-orphans
ExecStop=/usr/bin/docker compose -f /srv/llm/docker-compose.yml down

[Install]
WantedBy=multi-user.target
```

## Token Costs & Caching

- Add Redis in front for response cache of popular prompts.
- Pre‑warm with frequent system prompts to reduce first‑token latency.

## Security

- Put an auth proxy (Caddy/Traefik) with tokens in front of the API.
- Rate‑limit; add per‑IP and per‑token quotas.

---

This stack runs great for local apps and dev workflows while staying within the 24 GB total VRAM envelope.


## Deeper Tuning & Details

### Quantization Choices

- AWQ (Activation‑aware): great for 12–14B on 8 GB cards, minimal quality loss.
- GPTQ: mature, widely available weights; pick group size 128/128g, act‑order true.
- GGUF: ideal for llama.cpp CPU/offload; use Q5_K_M for balance.

Convert to AWQ (example):

```bash
pip install autoawq
python -m awq.cli.quant --model qwen2.5-14b-instruct --wbits 4 --true-sequential --groupsize 128 \
  --dump-awq --save-dir ./qwen2.5-14b-instruct-awq
```

### Memory Math (rule‑of‑thumb)

- 14B params × 2 bytes (FP16) ≈ 28 GB → too big. Quant(4‑bit) → ~7 GB + KV cache.
- KV cache per token per layer adds up; cap `--max-num-seqs` and use 8K context.

### vLLM Flags That Matter

- `--gpu-memory-utilization 0.9` — fill VRAM, but avoid OOM.
- `--tensor-parallel-size 3` — shard across your 3 GPUs.
- `--disable-log-requests` in prod to cut noisy logs.

### Benchmark Script

```bash
ab -n 50 -c 5 -p body.json -T application/json http://localhost:8000/v1/chat/completions
```

Body (`body.json`):

```json
{"model":"qwen2.5-14b-instruct-awq","messages":[{"role":"user","content":"Write a haiku about caching"}],"max_tokens":64}
```

### Tooling & RAG

- Put an embeddings service (text‑embedding‑3‑small, bge‑small) with pgvector/Qdrant.
- Keep chunks 350–600 tokens; store metadata for citations.

### Observability Tips

- Enable Prometheus on vLLM; dashboard: GPU util, tokens/sec, queue depth, latency p95.
- Use `nvidia-smi dmon -s pucvmet` for low‑overhead GPU sampling.

### Security Hardening

- Reverse proxy (Caddy/Traefik) with OAuth2/OIDC upstream; per‑token quotas.
- Strip prompts in logs; encrypt at rest the model store if needed.
